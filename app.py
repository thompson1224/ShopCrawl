from fastapi import FastAPI, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse, Response
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from sqlalchemy import desc
from fastapi import FastAPI, Request, Depends, HTTPException
from sqlalchemy.orm import Session
from auth import create_access_token, get_current_user, get_current_user_required, get_db
from models import User
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

import uvicorn
import asyncio
import os
import re
import httpx
from playwright.async_api import async_playwright
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from sqlalchemy.orm import Session
from models import HotDeal, SessionLocal
from datetime import datetime, timedelta
import logging
import pytz

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ì„ íƒ)
from dotenv import load_dotenv
load_dotenv()

# ë””ë²„ê¹…: í™˜ê²½ë³€ìˆ˜ í™•ì¸
print("=" * 50)
print("ğŸ” í™˜ê²½ë³€ìˆ˜ ë¡œë“œ í™•ì¸:")
print(f"SECRET_KEY: {os.getenv('SECRET_KEY', 'NOT_FOUND')[:20]}...")
print(f"NAVER_CLIENT_ID: {os.getenv('NAVER_CLIENT_ID', 'NOT_FOUND')}")
print(f"NAVER_CLIENT_SECRET: {os.getenv('NAVER_CLIENT_SECRET', 'NOT_FOUND')[:10]}...")
print("=" * 50)


# Railwayì—ì„œ ì œê³µí•˜ëŠ” PORT ì‚¬ìš© (ì—†ìœ¼ë©´ 8000)
PORT = int(os.getenv("PORT", 8000))

# ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°± URL (ë°°í¬ í›„ ë³€ê²½ í•„ìš”)
RAILWAY_URL = os.getenv("RAILWAY_PUBLIC_DOMAIN", "localhost:8000")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
NAVER_CALLBACK_URL = f"https://{RAILWAY_URL}/api/auth/naver/callback" if RAILWAY_URL != "localhost:8000" else "http://localhost:8000/api/auth/naver/callback"
# # ë„¤ì´ë²„ ì„¤ì •
# NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
# NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
# NAVER_CALLBACK_URL = "http://localhost:8000/api/auth/naver/callback"

# if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
#     print("âš ï¸ ê²½ê³ : ë„¤ì´ë²„ ë¡œê·¸ì¸ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì •ì  íŒŒì¼ ë§ˆìš´íŠ¸ (templates í´ë”)
try:
    app.mount("/static", StaticFiles(directory="templates"), name="static")
except:
    pass

templates = Jinja2Templates(directory="templates")

# CORS ì„¤ì •
@app.middleware("http")
async def add_csp_header(request: Request, call_next):
    response = await call_next(request)
    response.headers["Content-Security-Policy"] = ("default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.tailwindcss.com; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; img-src 'self' data: https:; connect-src 'self';")
    return response

app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
async def scrape_ppomppu():
    logger.info("ë½ë¿Œ í¬ë¡¤ë§ ì‹œì‘")
    url = 'https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu'
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10.0)
            response.raise_for_status()
    except httpx.RequestError: 
        logger.error("ë½ë¿Œ í¬ë¡¤ë§ ì‹¤íŒ¨")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    deal_list = []
    base_url = "https://www.ppomppu.co.kr/zboard/"
    main_table = soup.find('table', id='revolution_main_table')
    if not main_table: 
        return []
    
    for item in main_table.find_all('tr', class_='baseList'):
        try:
            title_cell = item.find('td', class_='title')
            author_cell = item.find('span', 'baseList-name')
            if not (title_cell and author_cell): continue
            title_tag = title_cell.find('a', class_='baseList-title')
            if title_tag and 'id=ppomppu' in title_tag['href']:
                full_title = title_tag.get_text(strip=True)
                link = base_url + title_tag['href'] if title_tag['href'].startswith("view.php") else title_tag['href']
                thumbnail_tag = title_cell.find('img')
                thumbnail_src = thumbnail_tag['src'] if thumbnail_tag else ""
                if thumbnail_src.startswith('//'): 
                    thumbnail = 'https:' + thumbnail_src
                else: 
                    thumbnail = thumbnail_src
                source = re.search(r'\[(.*?)\]', full_title).group(1) if re.search(r'\[(.*?)\]', full_title) else "ê¸°íƒ€"
                price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›)', full_title)
                price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                shipping = "ë¬´ë£Œë°°ì†¡" if "ë¬´ë£Œ" in full_title or "ë¬´ë°°" in full_title else "ë°°ì†¡ë¹„ ì •ë³´ ì—†ìŒ"
                clean_title = re.sub(r'\[.*?\]|(\d{1,3}(?:,\d{3})*ì›)|\s*\(?\d+\)?$|\s*/\s*ë¬´ë£Œë°°ì†¡|\s*/\s*ë¬´ë°°', '', full_title).strip()
                deal_list.append({'thumbnail': thumbnail, 'source': 'ë½ë¿Œ', 'author': author_cell.text.strip(), 'title': clean_title, 'price': price, 'shipping': shipping, 'link': link})
        except Exception: 
            continue
    
    logger.info(f"ë½ë¿Œ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

async def scrape_ruliweb():
    logger.info("ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    browser = None
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=['--disable-dev-shm-usage', '--no-sandbox'])
            page = await browser.new_page()
            await page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "stylesheet", "font"] else route.continue_())
            await page.goto('https://bbs.ruliweb.com/market/board/1020', wait_until='domcontentloaded', timeout=15000)
            
            list_selector = 'table.board_list_table tbody tr.table_body'
            await page.wait_for_selector(list_selector, timeout=10000)
            posts = await page.query_selector_all(list_selector)

            for item in posts:
                try:
                    is_notice = await item.evaluate('(element) => element.classList.contains("notice")')
                    if is_notice: continue

                    title_tag = await item.query_selector('a.deco')
                    if not title_tag: continue
                    
                    full_title = (await title_tag.inner_text()).strip()
                    link = await title_tag.get_attribute('href')
                    if link and link.startswith('/'): 
                        link = 'https://bbs.ruliweb.com' + link
                    
                    author_tag = await item.query_selector('td.writer a')
                    author = (await author_tag.inner_text()).strip() if author_tag else "ì‘ì„±ì"

                    thumbnail = ""
                    price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›|\d+\.\d+\$)', full_title)
                    price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                    clean_title = re.sub(r'\[.*?\]|\s*\(\d+\)$|\s*\(?(\d{1,3}(?:,\d{3})*ì›|\d+\.\d+\$)\)?', '', full_title).strip()
                    
                    deal_list.append({'thumbnail': thumbnail, 'source': 'ë£¨ë¦¬ì›¹', 'author': author, 'title': clean_title, 'price': price, 'shipping': 'ì •ë³´ ì—†ìŒ', 'link': link})
                except Exception: 
                    continue
            
            await browser.close()
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        if browser: 
            await browser.close()
    
    logger.info(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

async def scrape_zod():
    logger.info("Zod í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    browser = None
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled', '--no-sandbox', '--disable-dev-shm-usage'])
            context = await browser.new_context(user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            page = await context.new_page()
            await page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "stylesheet", "font"] else route.continue_())
            
            await page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined });")
            await page.goto('https://zod.kr/deal', wait_until='domcontentloaded', timeout=15000)
            await page.wait_for_timeout(2000)
            
            try:
                await page.wait_for_selector('ul.app-board-template-list', state='attached', timeout=8000)
            except: 
                pass
            
            posts = await page.query_selector_all('ul.app-board-template-list li')
            if not posts:
                posts = await page.query_selector_all('li[class*="app-list"]')
            
            for item in posts:
                try:
                    text_content = await item.inner_text()
                    if not text_content or 'ê³µì§€' in text_content[:10]: 
                        continue
                    
                    link_tag = await item.query_selector('a[href*="/deal/"]')
                    if not link_tag: 
                        continue
                    
                    href = await link_tag.get_attribute('href')
                    if not href or '/deal/' not in href: 
                        continue
                    link = 'https://zod.kr' + href if href.startswith('/') else href
                    
                    thumbnail = ""
                    img = await item.query_selector('img')
                    if img:
                        thumbnail_src = await img.get_attribute('src')
                        if thumbnail_src:
                            if thumbnail_src.startswith('//'):
                                thumbnail = 'https:' + thumbnail_src
                            elif thumbnail_src.startswith('http'):
                                thumbnail = thumbnail_src
                    
                    title = "ì œëª© ì—†ìŒ"
                    title_span = await item.query_selector('span.app-list-title-item')
                    if title_span:
                        title = await title_span.inner_text()
                        title = title.strip()
                    
                    price = "ê°€ê²© ì •ë³´ ì—†ìŒ"
                    strong_tags = await item.query_selector_all('strong')
                    for strong in strong_tags:
                        strong_text = await strong.inner_text()
                        if 'ì›' in strong_text or ',' in strong_text:
                            price = strong_text.strip()
                            break
                    
                    author = "ì‘ì„±ì"
                    member_div = await item.query_selector('div.app-list-member')
                    if member_div:
                        member_text = await member_div.inner_text()
                        if member_text:
                            author = member_text.strip().split('\n')[0]
                    
                    deal_list.append({'thumbnail': thumbnail, 'source': 'Zod', 'author': author, 'title': title, 'price': price, 'shipping': 'ì •ë³´ ì—†ìŒ', 'link': link})
                except Exception: 
                    continue
            
            await browser.close()
    except Exception as e:
        logger.error(f"Zod í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        if browser: 
            await browser.close()
    
    logger.info(f"Zod í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

#ì–´ë¯¸ìƒˆ í¬ë¡¤
async def scrape_eomisae():
    logger.info("ì–´ë¯¸ìƒˆ í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    browser = None
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True, 
                args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-blink-features=AutomationControlled']
            )
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            # JavaScript ê°ì§€ ìš°íšŒ
            await page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined });")
            
            logger.info("ì–´ë¯¸ìƒˆ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            await page.goto('https://eomisae.co.kr/fs', wait_until='networkidle', timeout=20000)
            await page.wait_for_timeout(3000)
            
            # í˜ì´ì§€ HTML í™•ì¸
            content = await page.content()
            logger.info(f"ì–´ë¯¸ìƒˆ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ, HTML ê¸¸ì´: {len(content)}")
            
            # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ëŒ€ê¸°
            try:
                await page.wait_for_selector('article, .card_el, .list-item', timeout=5000)
            except:
                logger.warning("ì–´ë¯¸ìƒˆ: ê²Œì‹œê¸€ ì…€ë ‰í„° ëŒ€ê¸° ì‹¤íŒ¨")
            
            # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
            posts = []
            selectors = [
                'article',
                '.card_el',
                'div[class*="card"]',
                'li[class*="item"]',
                '.list-item',
                '[data-post]'
            ]
            
            for selector in selectors:
                posts = await page.query_selector_all(selector)
                if posts:
                    logger.info(f"ì–´ë¯¸ìƒˆ: '{selector}' ì…€ë ‰í„°ë¡œ {len(posts)}ê°œ ë°œê²¬")
                    break
            
            if not posts:
                logger.warning("ì–´ë¯¸ìƒˆ: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                # HTML ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                logger.debug(f"ì–´ë¯¸ìƒˆ HTML ìƒ˜í”Œ: {content[:500]}")
                await browser.close()
                return []
            
            for idx, item in enumerate(posts[:20]):  # ìµœëŒ€ 20ê°œë§Œ
                try:
                    # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_content = await item.inner_text()
                    
                    # ë§í¬ ì°¾ê¸°
                    link_tag = await item.query_selector('a')
                    if not link_tag:
                        continue
                    
                    link_href = await link_tag.get_attribute('href')
                    if not link_href:
                        continue
                    
                    if link_href.startswith('/'):
                        link = 'https://eomisae.co.kr' + link_href
                    elif link_href.startswith('http'):
                        link = link_href
                    else:
                        link = 'https://eomisae.co.kr/' + link_href
                    
                    # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                    title = ""
                    title_selectors = ['h3', 'h2', '.title', '[class*="title"]', 'a']
                    for sel in title_selectors:
                        title_tag = await item.query_selector(sel)
                        if title_tag:
                            title_text = await title_tag.inner_text()
                            if title_text and len(title_text.strip()) > 3:
                                title = title_text.strip()
                                break
                    
                    if not title:
                        # í…ìŠ¤íŠ¸ì—ì„œ ì²« ì¤„ ì‚¬ìš©
                        lines = text_content.strip().split('\n')
                        title = lines[0][:100] if lines else f"ì–´ë¯¸ìƒˆ í•«ë”œ #{idx+1}"
                    
                    # ì‘ì„±ì
                    author = "ì‘ì„±ì"
                    author_selectors = ['.user', '.author', 'span[class*="user"]', 'span[class*="author"]']
                    for sel in author_selectors:
                        author_tag = await item.query_selector(sel)
                        if author_tag:
                            author_text = await author_tag.inner_text()
                            if author_text:
                                author = author_text.strip()
                                break
                    
                    # ì¸ë„¤ì¼
                    thumbnail = ""
                    img_tag = await item.query_selector('img')
                    if img_tag:
                        thumbnail_src = await img_tag.get_attribute('src')
                        if thumbnail_src:
                            if thumbnail_src.startswith('//'):
                                thumbnail = 'https:' + thumbnail_src
                            elif thumbnail_src.startswith('http'):
                                thumbnail = thumbnail_src
                            elif thumbnail_src.startswith('/'):
                                thumbnail = 'https://eomisae.co.kr' + thumbnail_src
                    
                    # ê°€ê²© ì¶”ì¶œ
                    price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›)', title)
                    price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                    
                    # ë°°ì†¡ë¹„
                    shipping = "ë¬´ë£Œë°°ì†¡" if "ë¬´ë£Œ" in title or "ë¬´ë°°" in title else "ì •ë³´ ì—†ìŒ"
                    
                    deal_list.append({
                        'thumbnail': thumbnail,
                        'source': 'ì–´ë¯¸ìƒˆ',
                        'author': author,
                        'title': title,
                        'price': price,
                        'shipping': shipping,
                        'link': link
                    })
                    
                    logger.debug(f"ì–´ë¯¸ìƒˆ í•­ëª© {idx+1}: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"ì–´ë¯¸ìƒˆ í•­ëª© {idx+1} íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            await browser.close()
            
    except Exception as e:
        logger.error(f"ì–´ë¯¸ìƒˆ í¬ë¡¤ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
        if browser:
            await browser.close()
    
    logger.info(f"ì–´ë¯¸ìƒˆ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list





# models.pyì—ë„ ì¶”ê°€
KST = pytz.timezone('Asia/Seoul')

async def crawl_and_save_to_db():
    logger.info(f"=== ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
    
    # ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰
    tasks = [scrape_ppomppu(), scrape_ruliweb(), scrape_zod(), scrape_eomisae()]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    all_deals = []
    for result in results:
        if not isinstance(result, Exception):
            all_deals.extend(result)
    
    # DBì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ ê°œì„ )
    db = SessionLocal()
    new_count = 0
    duplicate_count = 0
    error_count = 0
    
    try:
        for deal in all_deals:
            try:
                existing = db.query(HotDeal).filter(HotDeal.link == deal['link']).first()
                
                if existing:
                    # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                    existing.title = deal['title']
                    existing.price = deal['price']
                    existing.shipping = deal['shipping']
                    existing.thumbnail = deal['thumbnail']
                    duplicate_count += 1
                else:
                    # ìƒˆ ë°ì´í„° ì¶”ê°€
                    db_deal = HotDeal(**deal, created_at=datetime.now(KST).replace(tzinfo=None))
                    db.add(db_deal)
                    new_count += 1
                
                db.flush()
                
            except Exception as item_error:
                logger.warning(f"í•­ëª© ì €ì¥ ì‹¤íŒ¨: {deal.get('link', 'unknown')} - {str(item_error)}")
                error_count += 1
                continue
        
        db.commit()
        
        # DB ì „ì²´ ê°œìˆ˜ í™•ì¸
        total_count = db.query(HotDeal).count()
        logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {new_count}ê°œ, ì—…ë°ì´íŠ¸ {duplicate_count}ê°œ, ì˜¤ë¥˜ {error_count}ê°œ, ì „ì²´ {total_count}ê°œ - {datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')}")
        
    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì „ì²´ ì˜¤ë¥˜: {e}")
        db.rollback()
    finally:
        db.close()

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
scheduler = AsyncIOScheduler()

# FastAPI ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ í™œì„±í™”")
    
    # ì²« í¬ë¡¤ë§ ì‹¤í–‰
    await crawl_and_save_to_db()
    
    # 1ë¶„ë§ˆë‹¤ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ (í…ŒìŠ¤íŠ¸ìš©, ì‹¤ì œë¡œëŠ” 5ë¶„ ì¶”ì²œ)
    scheduler.add_job(crawl_and_save_to_db, 'interval', minutes=1, id='crawl_job')
    scheduler.start()
    logger.info("â° 1ë¶„ë§ˆë‹¤ ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ ë“±ë¡")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("ğŸ›‘ ì„œë²„ ì¢…ë£Œ: ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì§€")
    scheduler.shutdown()

# API ì—”ë“œí¬ì¸íŠ¸ (í˜ì´ì§€ë„¤ì´ì…˜ ì¶”ê°€)
@app.get('/api/hotdeals')
async def hotdeals(
    source: str = "all", 
    page: int = 1, 
    per_page: int = 20,
    db: Session = Depends(get_db)
):
    query = db.query(HotDeal)
    
    if source != "all":
        query = query.filter(HotDeal.source == source)
    
    # ì „ì²´ ê°œìˆ˜
    total = query.count()
    
    # id ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
    query = query.order_by(desc(HotDeal.id))
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    offset = (page - 1) * per_page
    deals = query.offset(offset).limit(per_page).all()
    
    # ì´ í˜ì´ì§€ ìˆ˜
    total_pages = (total + per_page - 1) // per_page
    
    logger.info(f"API ìš”ì²­: {source}, í˜ì´ì§€ {page}/{total_pages} - {len(deals)}ê°œ ë°˜í™˜")
    
    return {
        "deals": [deal.to_dict() for deal in deals],
        "pagination": {
            "page": page,
            "per_page": per_page,
            "total": total,
            "total_pages": total_pages
        }
    }

# ì „ì²´ í†µê³„ API (ì„ íƒ)
@app.get('/api/stats')
async def stats(db: Session = Depends(get_db)):
    total = db.query(HotDeal).count()
    ppomppu_count = db.query(HotDeal).filter(HotDeal.source == 'ë½ë¿Œ').count()
    ruliweb_count = db.query(HotDeal).filter(HotDeal.source == 'ë£¨ë¦¬ì›¹').count()
    zod_count = db.query(HotDeal).filter(HotDeal.source == 'Zod').count()
    eomisae_count = db.query(HotDeal).filter(HotDeal.source == 'ì–´ë¯¸ìƒˆ').count()
    
    return {
        "total": total,
        "ppomppu": ppomppu_count,
        "ruliweb": ruliweb_count,
        "zod": zod_count,
        "eomisae" : eomisae_count
    }

# ìˆ˜ë™ í¬ë¡¤ë§ API (í…ŒìŠ¤íŠ¸ìš©)
@app.post('/api/crawl-now')
async def manual_crawl():
    logger.info("ìˆ˜ë™ í¬ë¡¤ë§ ìš”ì²­")
    await crawl_and_save_to_db()
    return {"status": "í¬ë¡¤ë§ ì™„ë£Œ"}

# ì´ë¯¸ì§€ í”„ë¡ì‹œ
@app.get("/image-proxy")
async def image_proxy(url: str, source: str = "ë½ë¿Œ"):
    referer_map = { "ë½ë¿Œ": "https://www.ppomppu.co.kr/", "ë£¨ë¦¬ì›¹": "https://bbs.ruliweb.com/", "Zod": "https://zod.kr/", "ì–´ë¯¸ìƒˆ": "https://eomisae.co.kr/"}
    headers = { 'Referer': referer_map.get(source, "https://www.google.com/"), 'User-Agent': 'Mozilla/5.0' }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            media_type = response.headers.get('content-type', 'image/jpeg')
            return Response(content=response.content, media_type=media_type)
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ í”„ë¡ì‹œ ì˜¤ë¥˜: {e}")
            return Response(status_code=404)
        
# ë„¤ì´ë²„ ë¡œê·¸ì¸ ì‹œì‘
@app.get('/api/auth/naver/login')
async def naver_login():
    """ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    import secrets
    state = secrets.token_urlsafe(16)
    
    naver_auth_url = (
        f"https://nid.naver.com/oauth2.0/authorize"
        f"?response_type=code"
        f"&client_id={NAVER_CLIENT_ID}"
        f"&redirect_uri={NAVER_CALLBACK_URL}"
        f"&state={state}"
    )
    
    return {"url": naver_auth_url}

# ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°±
@app.get('/api/auth/naver/callback')
async def naver_callback(code: str, state: str, db: Session = Depends(get_db)):
    """ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°± ì²˜ë¦¬"""
    
    # 1. ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰
    token_url = "https://nid.naver.com/oauth2.0/token"
    token_params = {
        "grant_type": "authorization_code",
        "client_id": NAVER_CLIENT_ID,
        "client_secret": NAVER_CLIENT_SECRET,
        "code": code,
        "state": state
    }
    
    async with httpx.AsyncClient() as client:
        token_response = await client.post(token_url, params=token_params, timeout=10.0)
        token_data = token_response.json()
        
        if "access_token" not in token_data:
            raise HTTPException(status_code=400, detail="ë„¤ì´ë²„ ë¡œê·¸ì¸ ì‹¤íŒ¨")
        
        access_token = token_data["access_token"]
        
        # 2. ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {"Authorization": f"Bearer {access_token}"}
        
        user_response = await client.get(user_info_url, headers=headers, timeout=10.0)
        user_data = user_response.json()
        
        if user_data.get("resultcode") != "00":
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
        
        naver_user = user_data["response"]
        provider_id = naver_user["id"]
        email = naver_user.get("email", "")
        name = naver_user.get("name", "")
        profile_image = naver_user.get("profile_image", "")
        
        # 3. DBì—ì„œ ì‚¬ìš©ì ì°¾ê¸° ë˜ëŠ” ìƒì„±
        user = db.query(User).filter(
            User.provider == "naver",
            User.provider_id == provider_id
        ).first()
        
        if not user:
            # ì‹ ê·œ ì‚¬ìš©ì ìƒì„±
            user = User(
                username=f"naver_{provider_id[:10]}",
                email=email,
                provider="naver",
                provider_id=provider_id,
                profile_image=profile_image,
                hashed_password=""
            )
            db.add(user)
            db.commit()
            db.refresh(user)
        
        # 4. JWT í† í° ìƒì„±
        jwt_token = create_access_token(data={"sub": user.id})
        
        # 5. í”„ë¡ íŠ¸ì—”ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (í† í° ì „ë‹¬)
        frontend_url = f"http://localhost:8000/?token={jwt_token}"
        
        from fastapi.responses import RedirectResponse
        return RedirectResponse(url=frontend_url)

# í˜„ì¬ ìœ ì € ì •ë³´ ì¡°íšŒ
@app.get('/api/auth/me')
async def get_me(current_user: User = Depends(get_current_user_required)):
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ìœ ì € ì •ë³´"""
    return {
        "id": current_user.id,
        "username": current_user.username,
        "email": current_user.email,
        "provider": current_user.provider,
        "profile_image": current_user.profile_image,
        "created_at": current_user.created_at.strftime('%Y-%m-%d')
    }

@app.get('/api/auth/naver/callback')
async def naver_callback(code: str, state: str, db: Session = Depends(get_db)):
    """ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°± ì²˜ë¦¬"""
    
    print(f"ğŸ”µ ë„¤ì´ë²„ ì½œë°± ì‹œì‘: code={code[:10]}...")
    
    # ... (í† í° ë°œê¸‰ ì½”ë“œ)
    
    print(f"âœ… ë„¤ì´ë²„ ì•¡ì„¸ìŠ¤ í† í°: {access_token[:20]}...")
    
    # ... (ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°)
    
    print(f"âœ… ë„¤ì´ë²„ ì‚¬ìš©ì ì •ë³´: {naver_user}")
    
    # ... (DB ì €ì¥)
    
    print(f"âœ… ìœ ì € ìƒì„±/ì¡°íšŒ ì™„ë£Œ: {user.username}")
    
    # JWT í† í° ìƒì„±
    jwt_token = create_access_token(data={"sub": user.id})
    print(f"âœ… JWT í† í° ìƒì„±: {jwt_token[:30]}...")
    
    # í”„ë¡ íŠ¸ì—”ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    frontend_url = f"http://localhost:8000/?token={jwt_token}"
    
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url=frontend_url)



# ì •ì  íŒŒì¼ ì œê³µ
current_dir = os.path.dirname(os.path.abspath(__file__))
templates_dir = os.path.join(current_dir, "templates")

@app.get("/", response_class=FileResponse)
async def read_root():
    return os.path.join(templates_dir, "index.html")

app.mount("/", StaticFiles(directory=templates_dir, html=True), name="static")

if __name__ == '__main__':
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
