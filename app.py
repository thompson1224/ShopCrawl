from fastapi import FastAPI, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse, Response
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from sqlalchemy import desc
from fastapi import FastAPI, Request, Depends, HTTPException
from sqlalchemy.orm import Session
from auth import create_access_token, get_current_user, get_current_user_required, get_db
from models import User
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from sqlalchemy import or_

import uvicorn
import asyncio
import os
import re
import httpx
from playwright.async_api import async_playwright
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from sqlalchemy.orm import Session
from models import HotDeal, SessionLocal
from datetime import datetime, timedelta
import logging
import pytz
import shutil
import google.generativeai as genai


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ì„ íƒ)
from dotenv import load_dotenv
load_dotenv()

# ë””ë²„ê¹…: í™˜ê²½ë³€ìˆ˜ í™•ì¸
print("=" * 50)
print("ğŸ” í™˜ê²½ë³€ìˆ˜ ë¡œë“œ í™•ì¸:")
print(f"SECRET_KEY: {os.getenv('SECRET_KEY', 'NOT_FOUND')[:20]}...")
print(f"NAVER_CLIENT_ID: {os.getenv('NAVER_CLIENT_ID', 'NOT_FOUND')}")
print(f"NAVER_CLIENT_SECRET: {os.getenv('NAVER_CLIENT_SECRET', 'NOT_FOUND')[:10]}...")
print("=" * 50)

#LLM ê´€ë ¨ í‚¤
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
CHROMA_DB_DIR = "/data/chroma_db" if os.getenv("FLY_APP_NAME") else "./chroma_db"

# Railwayì—ì„œ ì œê³µí•˜ëŠ” PORT ì‚¬ìš© (ì—†ìœ¼ë©´ 8000)
PORT = int(os.getenv("PORT", 8000))

# ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°± URL
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
APP_NAME = os.getenv("FLY_APP_NAME") 
if APP_NAME:
    # Fly.io ë°°í¬ í™˜ê²½
    BASE_URL = f"https://{APP_NAME}.fly.dev"
    NAVER_CALLBACK_URL = f"{BASE_URL}/api/auth/naver/callback"
else:
    # ë¡œì»¬ í™˜ê²½ (localhost:8000)
    BASE_URL = "http://localhost:8000"
    NAVER_CALLBACK_URL = f"{BASE_URL}/api/auth/naver/callback"

# # ë„¤ì´ë²„ ì„¤ì •
# NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
# NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
# NAVER_CALLBACK_URL = "http://localhost:8000/api/auth/naver/callback"

# if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
#     print("âš ï¸ ê²½ê³ : ë„¤ì´ë²„ ë¡œê·¸ì¸ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ì •ì  íŒŒì¼ ë§ˆìš´íŠ¸ (templates í´ë”)
try:
    app.mount("/static", StaticFiles(directory="templates"), name="static")
except:
    pass

templates = Jinja2Templates(directory="templates")

# CORS ì„¤ì •
@app.middleware("http")
async def add_csp_header(request: Request, call_next):
    response = await call_next(request)
    response.headers["Content-Security-Policy"] = ("default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.tailwindcss.com; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; img-src 'self' data: https:; connect-src 'self';")
    return response

app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])


# í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
async def scrape_ppomppu():
    logger.info("ë½ë¿Œ í¬ë¡¤ë§ ì‹œì‘")
    url = 'https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu'
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10.0)
            response.raise_for_status()
    except httpx.RequestError: 
        logger.error("ë½ë¿Œ í¬ë¡¤ë§ ì‹¤íŒ¨")
        return []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    deal_list = []
    base_url = "https://www.ppomppu.co.kr/zboard/"
    main_table = soup.find('table', id='revolution_main_table')
    if not main_table: 
        return []
    
    for item in main_table.find_all('tr', class_='baseList'):
        try:
            title_cell = item.find('td', class_='title')
            author_cell = item.find('span', 'baseList-name')
            if not (title_cell and author_cell): continue
            title_tag = title_cell.find('a', class_='baseList-title')
            if title_tag and 'id=ppomppu' in title_tag['href']:
                full_title = title_tag.get_text(strip=True)
                link = base_url + title_tag['href'] if title_tag['href'].startswith("view.php") else title_tag['href']
                thumbnail_tag = title_cell.find('img')
                thumbnail_src = thumbnail_tag['src'] if thumbnail_tag else ""
                if thumbnail_src.startswith('//'): 
                    thumbnail = 'https:' + thumbnail_src
                else: 
                    thumbnail = thumbnail_src
                source = re.search(r'\[(.*?)\]', full_title).group(1) if re.search(r'\[(.*?)\]', full_title) else "ê¸°íƒ€"
                price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›)', full_title)
                price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                shipping = "ë¬´ë£Œë°°ì†¡" if "ë¬´ë£Œ" in full_title or "ë¬´ë°°" in full_title else "ë°°ì†¡ë¹„ ì •ë³´ ì—†ìŒ"
                clean_title = re.sub(r'\[.*?\]|(\d{1,3}(?:,\d{3})*ì›)|\s*\(?\d+\)?$|\s*/\s*ë¬´ë£Œë°°ì†¡|\s*/\s*ë¬´ë°°', '', full_title).strip()
                deal_list.append({'thumbnail': thumbnail, 'source': 'ë½ë¿Œ', 'author': author_cell.text.strip(), 'title': clean_title, 'price': price, 'shipping': shipping, 'link': link})
        except Exception: 
            continue
    
    logger.info(f"ë½ë¿Œ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

async def scrape_ruliweb():
    logger.info("ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    browser = None
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=['--disable-dev-shm-usage', '--no-sandbox'])
            page = await browser.new_page()
            await page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "stylesheet", "font"] else route.continue_())
            await page.goto('https://bbs.ruliweb.com/market/board/1020', wait_until='domcontentloaded', timeout=15000)
            
            list_selector = 'table.board_list_table tbody tr.table_body'
            await page.wait_for_selector(list_selector, timeout=10000)
            posts = await page.query_selector_all(list_selector)

            for item in posts:
                try:
                    is_notice = await item.evaluate('(element) => element.classList.contains("notice")')
                    if is_notice: continue

                    title_tag = await item.query_selector('a.deco')
                    if not title_tag: continue
                    
                    full_title = (await title_tag.inner_text()).strip()
                    link = await title_tag.get_attribute('href')
                    if link and link.startswith('/'): 
                        link = 'https://bbs.ruliweb.com' + link
                    
                    author_tag = await item.query_selector('td.writer a')
                    author = (await author_tag.inner_text()).strip() if author_tag else "ì‘ì„±ì"

                    thumbnail = ""
                    price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›|\d+\.\d+\$)', full_title)
                    price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                    clean_title = re.sub(r'\[.*?\]|\s*\(\d+\)$|\s*\(?(\d{1,3}(?:,\d{3})*ì›|\d+\.\d+\$)\)?', '', full_title).strip()
                    
                    deal_list.append({'thumbnail': thumbnail, 'source': 'ë£¨ë¦¬ì›¹', 'author': author, 'title': clean_title, 'price': price, 'shipping': 'ì •ë³´ ì—†ìŒ', 'link': link})
                except Exception: 
                    continue
            
            await browser.close()
    except Exception as e:
        logger.error(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        if browser: 
            await browser.close()
    
    logger.info(f"ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

async def scrape_zod():
    logger.info("Zod í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    browser = None
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=['--disable-blink-features=AutomationControlled', '--no-sandbox', '--disable-dev-shm-usage'])
            context = await browser.new_context(user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            page = await context.new_page()
            await page.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "stylesheet", "font"] else route.continue_())
            
            await page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined });")
            await page.goto('https://zod.kr/deal', wait_until='domcontentloaded', timeout=15000)
            await page.wait_for_timeout(2000)
            
            try:
                await page.wait_for_selector('ul.app-board-template-list', state='attached', timeout=8000)
            except: 
                pass
            
            posts = await page.query_selector_all('ul.app-board-template-list li')
            if not posts:
                posts = await page.query_selector_all('li[class*="app-list"]')
            
            for item in posts:
                try:
                    text_content = await item.inner_text()
                    if not text_content or 'ê³µì§€' in text_content[:10]: 
                        continue
                    
                    link_tag = await item.query_selector('a[href*="/deal/"]')
                    if not link_tag: 
                        continue
                    
                    href = await link_tag.get_attribute('href')
                    if not href or '/deal/' not in href: 
                        continue
                    link = 'https://zod.kr' + href if href.startswith('/') else href
                    
                    thumbnail = ""
                    img = await item.query_selector('img')
                    if img:
                        thumbnail_src = await img.get_attribute('src')
                        if thumbnail_src:
                            if thumbnail_src.startswith('//'):
                                thumbnail = 'https:' + thumbnail_src
                            elif thumbnail_src.startswith('http'):
                                thumbnail = thumbnail_src
                    
                    title = "ì œëª© ì—†ìŒ"
                    title_span = await item.query_selector('span.app-list-title-item')
                    if title_span:
                        title = await title_span.inner_text()
                        title = title.strip()
                    
                    price = "ê°€ê²© ì •ë³´ ì—†ìŒ"
                    strong_tags = await item.query_selector_all('strong')
                    for strong in strong_tags:
                        strong_text = await strong.inner_text()
                        if 'ì›' in strong_text or ',' in strong_text:
                            price = strong_text.strip()
                            break
                    
                    author = "ì‘ì„±ì"
                    member_div = await item.query_selector('div.app-list-member')
                    if member_div:
                        member_text = await member_div.inner_text()
                        if member_text:
                            author = member_text.strip().split('\n')[0]
                    
                    deal_list.append({'thumbnail': thumbnail, 'source': 'Zod', 'author': author, 'title': title, 'price': price, 'shipping': 'ì •ë³´ ì—†ìŒ', 'link': link})
                except Exception: 
                    continue
            
            await browser.close()
    except Exception as e:
        logger.error(f"Zod í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        if browser: 
            await browser.close()
    
    logger.info(f"Zod í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

# í€˜ì´ì‚¬ì¡´ í¬ë¡¤
async def scrape_quasarzone():
    logger.info("í€˜ì´ì‚¬ì¡´ í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    
    try:
        # httpxë¡œ ê°„ë‹¨í•˜ê²Œ í¬ë¡¤ë§ (ì •ì  HTML)
        async with httpx.AsyncClient() as client:
            response = await client.get(
                'https://quasarzone.com/bbs/qb_saleinfo',
                headers={
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                    'Referer': 'https://quasarzone.com'
                },
                timeout=15.0
            )
            response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        logger.info(f"í€˜ì´ì‚¬ì¡´ HTML ê¸¸ì´: {len(response.text)}")
        
        # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
        posts = soup.find_all('div', class_='market-info-list')
        logger.info(f"í€˜ì´ì‚¬ì¡´: {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        for idx, item in enumerate(posts[:20]):  # ìµœëŒ€ 20ê°œ
            try:
                # ì¸ë„¤ì¼
                thumbnail = ""
                thumb_wrap = item.find('div', class_='thumb-wrap')
                if thumb_wrap:
                    img_tag = thumb_wrap.find('img', class_='maxImg')
                    if img_tag and img_tag.get('src'):
                        thumbnail_src = img_tag['src']
                        if thumbnail_src.startswith('//'):
                            thumbnail = 'https:' + thumbnail_src
                        elif thumbnail_src.startswith('http'):
                            thumbnail = thumbnail_src
                        elif thumbnail_src.startswith('/'):
                            thumbnail = 'https://quasarzone.com' + thumbnail_src
                
                # ì œëª© ë° ë§í¬
                cont = item.find('div', class_='market-info-list-cont')
                if not cont:
                    continue
                
                tit = cont.find('p', class_='tit')
                if not tit:
                    continue
                
                link_tag = tit.find('a', class_='subject-link')
                if not link_tag:
                    continue
                
                title = link_tag.get_text(strip=True)
                href = link_tag.get('href', '')
                
                if href.startswith('/'):
                    link = 'https://quasarzone.com' + href
                elif href.startswith('http'):
                    link = href
                else:
                    link = 'https://quasarzone.com/' + href
                
                # ì‘ì„±ì
                author = "ì‘ì„±ì"
                nick_wrap = cont.find('span', class_='nick')
                if nick_wrap:
                    author = nick_wrap.get_text(strip=True)
                
                # ê°€ê²© ì¶”ì¶œ (ì œëª©ì—ì„œ)
                price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›)', title)
                price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                
                # ë°°ì†¡ë¹„
                shipping = "ë¬´ë£Œë°°ì†¡" if "ë¬´ë£Œ" in title or "ë¬´ë°°" in title else "ì •ë³´ ì—†ìŒ"
                
                deal_list.append({
                    'thumbnail': thumbnail,
                    'source': 'í€˜ì´ì‚¬ì¡´',
                    'author': author,
                    'title': title,
                    'price': price,
                    'shipping': shipping,
                    'link': link
                })
                
                logger.debug(f"í€˜ì´ì‚¬ì¡´ í•­ëª© {idx+1}: {title[:30]}...")
                
            except Exception as e:
                logger.warning(f"í€˜ì´ì‚¬ì¡´ í•­ëª© {idx+1} íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        logger.error(f"í€˜ì´ì‚¬ì¡´ í¬ë¡¤ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
    
    logger.info(f"í€˜ì´ì‚¬ì¡´ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list

#ì–´ë¯¸ìƒˆ í¬ë¡¤
async def scrape_eomisae():
    logger.info("ì–´ë¯¸ìƒˆ í¬ë¡¤ë§ ì‹œì‘")
    deal_list = []
    browser = None
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True, 
                args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-blink-features=AutomationControlled']
            )
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            # JavaScript ê°ì§€ ìš°íšŒ
            await page.add_init_script("Object.defineProperty(navigator, 'webdriver', { get: () => undefined });")
            
            logger.info("ì–´ë¯¸ìƒˆ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            await page.goto('https://eomisae.co.kr/fs', wait_until='networkidle', timeout=20000)
            await page.wait_for_timeout(3000)
            
            # í˜ì´ì§€ HTML í™•ì¸
            content = await page.content()
            logger.info(f"ì–´ë¯¸ìƒˆ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ, HTML ê¸¸ì´: {len(content)}")
            
            # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ëŒ€ê¸°
            try:
                await page.wait_for_selector('article, .card_el, .list-item', timeout=5000)
            except:
                logger.warning("ì–´ë¯¸ìƒˆ: ê²Œì‹œê¸€ ì…€ë ‰í„° ëŒ€ê¸° ì‹¤íŒ¨")
            
            # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
            posts = []
            selectors = [
                'article',
                '.card_el',
                'div[class*="card"]',
                'li[class*="item"]',
                '.list-item',
                '[data-post]'
            ]
            
            for selector in selectors:
                posts = await page.query_selector_all(selector)
                if posts:
                    logger.info(f"ì–´ë¯¸ìƒˆ: '{selector}' ì…€ë ‰í„°ë¡œ {len(posts)}ê°œ ë°œê²¬")
                    break
            
            if not posts:
                logger.warning("ì–´ë¯¸ìƒˆ: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                # HTML ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                logger.debug(f"ì–´ë¯¸ìƒˆ HTML ìƒ˜í”Œ: {content[:500]}")
                await browser.close()
                return []
            
            for idx, item in enumerate(posts[:20]):  # ìµœëŒ€ 20ê°œë§Œ
                try:
                    # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_content = await item.inner_text()
                    
                    # ë§í¬ ì°¾ê¸°
                    link_tag = await item.query_selector('a')
                    if not link_tag:
                        continue
                    
                    link_href = await link_tag.get_attribute('href')
                    if not link_href:
                        continue
                    
                    if link_href.startswith('/'):
                        link = 'https://eomisae.co.kr' + link_href
                    elif link_href.startswith('http'):
                        link = link_href
                    else:
                        link = 'https://eomisae.co.kr/' + link_href
                    
                    # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                    title = ""
                    title_selectors = ['h3', 'h2', '.title', '[class*="title"]', 'a']
                    for sel in title_selectors:
                        title_tag = await item.query_selector(sel)
                        if title_tag:
                            title_text = await title_tag.inner_text()
                            if title_text and len(title_text.strip()) > 3:
                                title = title_text.strip()
                                break
                    
                    if not title:
                        # í…ìŠ¤íŠ¸ì—ì„œ ì²« ì¤„ ì‚¬ìš©
                        lines = text_content.strip().split('\n')
                        title = lines[0][:100] if lines else f"ì–´ë¯¸ìƒˆ í•«ë”œ #{idx+1}"
                    
                    # ì‘ì„±ì
                    author = "ì‘ì„±ì"
                    author_selectors = ['.user', '.author', 'span[class*="user"]', 'span[class*="author"]']
                    for sel in author_selectors:
                        author_tag = await item.query_selector(sel)
                        if author_tag:
                            author_text = await author_tag.inner_text()
                            if author_text:
                                author = author_text.strip()
                                break
                    
                    # ì¸ë„¤ì¼
                    thumbnail = ""
                    img_tag = await item.query_selector('img')
                    if img_tag:
                        thumbnail_src = await img_tag.get_attribute('src')
                        if thumbnail_src:
                            if thumbnail_src.startswith('//'):
                                thumbnail = 'https:' + thumbnail_src
                            elif thumbnail_src.startswith('http'):
                                thumbnail = thumbnail_src
                            elif thumbnail_src.startswith('/'):
                                thumbnail = 'https://eomisae.co.kr' + thumbnail_src
                    
                    # ê°€ê²© ì¶”ì¶œ
                    price_match = re.search(r'(\d{1,3}(?:,\d{3})*ì›)', title)
                    price = price_match.group(1) if price_match else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                    
                    # ë°°ì†¡ë¹„
                    shipping = "ë¬´ë£Œë°°ì†¡" if "ë¬´ë£Œ" in title or "ë¬´ë°°" in title else "ì •ë³´ ì—†ìŒ"
                    
                    deal_list.append({
                        'thumbnail': thumbnail,
                        'source': 'ì–´ë¯¸ìƒˆ',
                        'author': author,
                        'title': title,
                        'price': price,
                        'shipping': shipping,
                        'link': link
                    })
                    
                    logger.debug(f"ì–´ë¯¸ìƒˆ í•­ëª© {idx+1}: {title[:30]}...")
                    
                except Exception as e:
                    logger.warning(f"ì–´ë¯¸ìƒˆ í•­ëª© {idx+1} íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            await browser.close()
            
    except Exception as e:
        logger.error(f"ì–´ë¯¸ìƒˆ í¬ë¡¤ë§ ì „ì²´ ì˜¤ë¥˜: {e}")
        if browser:
            await browser.close()
    
    logger.info(f"ì–´ë¯¸ìƒˆ í¬ë¡¤ë§ ì™„ë£Œ: {len(deal_list)}ê°œ")
    return deal_list





# models.pyì—ë„ ì¶”ê°€
KST = pytz.timezone('Asia/Seoul')

async def crawl_and_save_to_db():
    logger.info(f"=== ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===")
    
    all_deals = []
    
    # --- 1. ê°€ë²¼ìš´ httpx ì‘ì—… (ë³‘ë ¬ ì‹¤í–‰) ---
    logger.info("--- 1ë‹¨ê³„: httpx í¬ë¡¤ëŸ¬ (ë³‘ë ¬) ì‹œì‘ ---")
    httpx_tasks = [scrape_ppomppu(), scrape_quasarzone()]
    results_httpx = await asyncio.gather(*httpx_tasks, return_exceptions=True)
    
    for result in results_httpx:
        if isinstance(result, Exception):
            logger.error(f"httpx í¬ë¡¤ë§ ì˜¤ë¥˜: {result}")
        else:
            all_deals.extend(result)
    logger.info("--- 1ë‹¨ê³„: httpx í¬ë¡¤ëŸ¬ ì™„ë£Œ ---")

    # --- 2. ë¬´ê±°ìš´ Playwright ì‘ì—… (ìˆœì°¨ ì‹¤í–‰) ---
    playwright_scrapers = [scrape_ruliweb, scrape_zod, scrape_eomisae]
    for scraper_func in playwright_scrapers:
        try:
            result_pw = await scraper_func() 
            if result_pw:
                all_deals.extend(result_pw)
        except Exception as e:
            logger.error(f"Playwright ì‘ì—… ({scraper_func.__name__}) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    logger.info("--- 2ë‹¨ê³„: Playwright í¬ë¡¤ëŸ¬ ì™„ë£Œ ---")

    # --- 3. DB ë° ë²¡í„° DB ì €ì¥ ---
    if not all_deals:
        return

    db = SessionLocal()
    new_count = 0
    duplicate_count = 0
    
    # RAG(ë²¡í„°DB)ì— ì¶”ê°€í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    new_deals_for_rag = []

    try:
        for deal in all_deals:
            try:
                existing = db.query(HotDeal).filter(HotDeal.link == deal['link']).first()
                
                if existing:
                    existing.title = deal['title']
                    existing.price = deal['price']
                    existing.shipping = deal['shipping']
                    existing.thumbnail = deal['thumbnail']
                    duplicate_count += 1
                else:
                    db_deal = HotDeal(**deal, created_at=datetime.now(KST).replace(tzinfo=None))
                    db.add(db_deal)
                    new_count += 1
                    
                    # [RAG] ì‹ ê·œ í•«ë”œì„ ë²¡í„° ë¬¸ì„œë¡œ ë³€í™˜
                    new_deals_for_rag.append(
                        Document(
                            page_content=f"[{deal['source']}] {deal['title']} - ê°€ê²©: {deal['price']}",
                            metadata={"link": deal['link'], "source": deal['source'], "price": deal['price']}
                        )
                    )
                
                db.flush()
            except Exception:
                continue
        
        db.commit()
        
        # --- 4. ë²¡í„° DB(Chroma)ì— ì‹ ê·œ ë°ì´í„° ì¶”ê°€ ---
        if new_deals_for_rag and GOOGLE_API_KEY:
            try:
                vectorstore = get_vectorstore()
                if vectorstore:
                    vectorstore.add_documents(new_deals_for_rag)
                    logger.info(f"ğŸ§  RAG: ì‹ ê·œ í•«ë”œ {len(new_deals_for_rag)}ê°œë¥¼ Gemini ê¸°ì–µì¥ì¹˜ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as rag_error:
                logger.error(f"ğŸ§  RAG ì €ì¥ ì‹¤íŒ¨: {rag_error}")

        total_count = db.query(HotDeal).count()
        logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: ì‹ ê·œ {new_count}, ì „ì²´ {total_count}")
        
    except Exception as e:
        logger.error(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")
        db.rollback()
    finally:
        db.close()

def backup_database():
    """DB ë°±ì—… (Railway Volume ë‚´ë¶€ì— ì €ì¥)"""
    if os.getenv("shopcrawl"):
        db_path = "/data/hotdeals.db"
        backup_dir = "/data/backups"
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(backup_dir, exist_ok=True)
        
        timestamp = datetime.now(KST).strftime('%Y%m%d_%H%M%S')
        backup_path = f"{backup_dir}/hotdeals_backup_{timestamp}.db"
        
        try:
            shutil.copy2(db_path, backup_path)
            logger.info(f"âœ… DB ë°±ì—… ì™„ë£Œ: {backup_path}")
            
            # ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ (ìµœê·¼ 7ê°œë§Œ ìœ ì§€)
            backups = sorted(
                [f for f in os.listdir(backup_dir) if f.startswith("hotdeals_backup_")],
                reverse=True
            )
            for old_backup in backups[7:]:
                old_path = os.path.join(backup_dir, old_backup)
                os.remove(old_path)
                logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup}")
                
        except Exception as e:
            logger.error(f"âŒ DB ë°±ì—… ì‹¤íŒ¨: {e}")
    else:
        logger.info("â­ï¸ ë¡œì»¬ í™˜ê²½: DB ë°±ì—… ìŠ¤í‚µ")

#Vector store
def get_vectorstore():
    """ë²¡í„° DB(ê¸°ì–µì¥ì¹˜) ê°€ì ¸ì˜¤ê¸° - Gemini ë²„ì „"""
    if not GOOGLE_API_KEY:
        return None
    
    # êµ¬ê¸€ì˜ ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/text-embedding-004", 
        google_api_key=GOOGLE_API_KEY
    )
    
    vectorstore = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embeddings,
        collection_name="hotdeals"
    )
    return vectorstore

# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
scheduler = AsyncIOScheduler()


# FastAPI ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# FastAPI ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ í™œì„±í™”")
    
    # ì„œë²„ ì‹œì‘ í›„ 5ì´ˆ ë’¤ ì²« í¬ë¡¤ë§ (Railway í—¬ìŠ¤ì²´í¬ í†µê³¼ ìœ„í•´)
    scheduler.add_job(
        crawl_and_save_to_db, 
        'date', 
        run_date=datetime.now(KST) + timedelta(seconds=5),
        id='first_crawl',
        timezone=KST
    )
    
    # 1ë¶„ë§ˆë‹¤ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„
    scheduler.add_job(
        crawl_and_save_to_db, 
        'interval', 
        minutes=5, 
        id='crawl_job',
        timezone=KST
    )
    
    # ë§¤ì¼ ìƒˆë²½ 3ì‹œ DB ë°±ì—… (ì¶”ê°€)
    scheduler.add_job(
        backup_database, 
        'cron', 
        hour=3, 
        minute=0,
        id='backup_job',
        timezone=KST
    )
    
    scheduler.start()
    logger.info("â° ì„œë²„ ì‹œì‘ 5ì´ˆ í›„ ì²« í¬ë¡¤ë§, ì´í›„ 1ë¶„ë§ˆë‹¤ ìë™ í¬ë¡¤ë§")
    logger.info("ğŸ’¾ ë§¤ì¼ ìƒˆë²½ 3ì‹œ DB ìë™ ë°±ì—… í™œì„±í™”")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("ğŸ›‘ ì„œë²„ ì¢…ë£Œ: ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì§€")
    scheduler.shutdown()

# API ì—”ë“œí¬ì¸íŠ¸ (í˜ì´ì§€ë„¤ì´ì…˜ ì¶”ê°€)
@app.get('/api/hotdeals')
async def hotdeals(
    source: str = "all", 
    page: int = 1, 
    per_page: int = 20,
    db: Session = Depends(get_db)
):
    query = db.query(HotDeal)
    
    if source != "all":
        query = query.filter(HotDeal.source == source)
    
    # ì „ì²´ ê°œìˆ˜
    total = query.count()
    
    # id ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
    query = query.order_by(desc(HotDeal.created_at))    
    # í˜ì´ì§€ë„¤ì´ì…˜
    offset = (page - 1) * per_page
    deals = query.offset(offset).limit(per_page).all()
    
    # ì´ í˜ì´ì§€ ìˆ˜
    total_pages = (total + per_page - 1) // per_page
    
    logger.info(f"API ìš”ì²­: {source}, í˜ì´ì§€ {page}/{total_pages} - {len(deals)}ê°œ ë°˜í™˜")
    
    return {
        "deals": [deal.to_dict() for deal in deals],
        "pagination": {
            "page": page,
            "per_page": per_page,
            "total": total,
            "total_pages": total_pages
        }
    }

# ì „ì²´ í†µê³„ API (ì„ íƒ)
@app.get('/api/stats')
async def stats(db: Session = Depends(get_db)):
    total = db.query(HotDeal).count()
    ppomppu_count = db.query(HotDeal).filter(HotDeal.source == 'ë½ë¿Œ').count()
    ruliweb_count = db.query(HotDeal).filter(HotDeal.source == 'ë£¨ë¦¬ì›¹').count()
    zod_count = db.query(HotDeal).filter(HotDeal.source == 'Zod').count()
    eomisae_count = db.query(HotDeal).filter(HotDeal.source == 'ì–´ë¯¸ìƒˆ').count()
    quasarzone_count = db.query(HotDeal).filter(HotDeal.source == 'í€˜ì´ì‚¬ì¡´').count()
    
    return {
        "total": total,
        "ppomppu": ppomppu_count,
        "ruliweb": ruliweb_count,
        "zod": zod_count,
        "eomisae" : eomisae_count,
        "quasarzone": quasarzone_count
    }

# ìˆ˜ë™ í¬ë¡¤ë§ API (í…ŒìŠ¤íŠ¸ìš©)
@app.post('/api/crawl-now')
async def manual_crawl():
    logger.info("ìˆ˜ë™ í¬ë¡¤ë§ ìš”ì²­")
    await crawl_and_save_to_db()
    return {"status": "í¬ë¡¤ë§ ì™„ë£Œ"}

# ì´ë¯¸ì§€ í”„ë¡ì‹œ
@app.get("/image-proxy")
async def image_proxy(url: str, source: str = "ë½ë¿Œ"):
    referer_map = { "ë½ë¿Œ": "https://www.ppomppu.co.kr/", "ë£¨ë¦¬ì›¹": "https://bbs.ruliweb.com/", "Zod": "https://zod.kr/", "ì–´ë¯¸ìƒˆ": "https://eomisae.co.kr/", "í€˜ì´ì‚¬ì¡´": "https://quasarzone.com/"}
    headers = { 'Referer': referer_map.get(source, "https://www.google.com/"), 'User-Agent': 'Mozilla/5.0' }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            media_type = response.headers.get('content-type', 'image/jpeg')
            return Response(content=response.content, media_type=media_type)
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ í”„ë¡ì‹œ ì˜¤ë¥˜: {e}")
            return Response(status_code=404)
        
# ë„¤ì´ë²„ ë¡œê·¸ì¸ ì‹œì‘
@app.get('/api/auth/naver/login')
async def naver_login():
    """ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    import secrets
    state = secrets.token_urlsafe(16)
    
    naver_auth_url = (
        f"https://nid.naver.com/oauth2.0/authorize"
        f"?response_type=code"
        f"&client_id={NAVER_CLIENT_ID}"
        f"&redirect_uri={NAVER_CALLBACK_URL}"
        f"&state={state}"
    )
    
    return {"url": naver_auth_url}

# ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°±
@app.get('/api/auth/naver/callback')
async def naver_callback(code: str, state: str, db: Session = Depends(get_db)):
    """ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°± ì²˜ë¦¬"""
    
    # 1. ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰
    token_url = "https://nid.naver.com/oauth2.0/token"
    token_params = {
        "grant_type": "authorization_code",
        "client_id": NAVER_CLIENT_ID,
        "client_secret": NAVER_CLIENT_SECRET,
        "code": code,
        "state": state
    }
    
    async with httpx.AsyncClient() as client:
        token_response = await client.post(token_url, params=token_params, timeout=10.0)
        token_data = token_response.json()
        
        if "access_token" not in token_data:
            raise HTTPException(status_code=400, detail="ë„¤ì´ë²„ ë¡œê·¸ì¸ ì‹¤íŒ¨")
        
        access_token = token_data["access_token"]
        
        # 2. ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        user_info_url = "https://openapi.naver.com/v1/nid/me"
        headers = {"Authorization": f"Bearer {access_token}"}
        
        user_response = await client.get(user_info_url, headers=headers, timeout=10.0)
        user_data = user_response.json()
        
        if user_data.get("resultcode") != "00":
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
        
        naver_user = user_data["response"]
        provider_id = naver_user["id"]
        email = naver_user.get("email", "")
        name = naver_user.get("name", "")
        profile_image = naver_user.get("profile_image", "")
        
        # 3. DBì—ì„œ ì‚¬ìš©ì ì°¾ê¸° ë˜ëŠ” ìƒì„±
        user = db.query(User).filter(
            User.provider == "naver",
            User.provider_id == provider_id
        ).first()
        
        if not user:
            # ì‹ ê·œ ì‚¬ìš©ì ìƒì„±
            user = User(
                username=f"naver_{provider_id[:10]}",
                email=email,
                provider="naver",
                provider_id=provider_id,
                profile_image=profile_image,
                hashed_password=""
            )
            db.add(user)
            db.commit()
            db.refresh(user)
        
        # 4. JWT í† í° ìƒì„±
        jwt_token = create_access_token(data={"sub": user.id})
        
        # 5. í”„ë¡ íŠ¸ì—”ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (í† í° ì „ë‹¬)
        frontend_url = f"{BASE_URL}/?token={jwt_token}"
        
        from fastapi.responses import RedirectResponse
        return RedirectResponse(url=frontend_url)

# --- [ë””ë²„ê¹…ìš©] ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ í™•ì¸ ---
@app.get("/api/debug/models")
async def list_available_models():
    if not GOOGLE_API_KEY:
        return {"error": "API Key ì—†ìŒ"}
    
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                models.append(m.name)
        return {"available_models": models}
    except Exception as e:
        return {"error": str(e)}

#AI ê²€ìƒ‰ API    
# --- AI ê²€ìƒ‰ API (Gemini) ---
@app.get("/api/search/ai")
async def search_ai(query: str, db: Session = Depends(get_db)):
    """
    [RAG ê³ ë„í™”] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)
    """
    if not query:
        return {"answer": "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}
    
    if not GOOGLE_API_KEY:
        return {"answer": "ì„œë²„ì— Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

    try:
        # --- 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜) ---
        vector_docs = []
        vectorstore = get_vectorstore()
        if vectorstore:
            retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
            vector_docs = retriever.invoke(query)
        
        # --- 2ë‹¨ê³„: í‚¤ì›Œë“œ ê²€ìƒ‰ (ì •í™•ì„± ê¸°ë°˜ - SQLite) ---
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê³µë°±ìœ¼ë¡œ ìª¼ê°œì„œ í‚¤ì›Œë“œë¡œ í™œìš© (ë‹¨ìˆœí™”ëœ ë°©ì‹)
        # ì˜ˆ: "4070 ëª¨ë‹ˆí„°" -> ["4070", "ëª¨ë‹ˆí„°"]
        keywords = query.split()
        keyword_deals = []
        
        if keywords:
            # ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì œëª©ì„ ì°¾ìŒ (AND ì¡°ê±´)
            sql_query = db.query(HotDeal)
            for word in keywords:
                sql_query = sql_query.filter(HotDeal.title.like(f"%{word}%"))
            
            # ìµœì‹ ìˆœ 5ê°œ
            keyword_deals = sql_query.order_by(desc(HotDeal.created_at)).limit(5).all()

        # --- 3ë‹¨ê³„: ê²°ê³¼ ë³‘í•© (Hybrid) ë° ì¤‘ë³µ ì œê±° ---
        # ë²¡í„° ê²°ê³¼ì™€ í‚¤ì›Œë“œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤.
        combined_results = {} # ë§í¬ë¥¼ í‚¤(Key)ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
        
        # 3-1. ë²¡í„° ê²°ê³¼ ì¶”ê°€
        for doc in vector_docs:
            link = doc.metadata.get('link')
            if link:
                combined_results[link] = {
                    "content": doc.page_content,
                    "link": link,
                    "source": "AIì¶”ì²œ"
                }
        
        # 3-2. í‚¤ì›Œë“œ ê²°ê³¼ ì¶”ê°€
        for deal in keyword_deals:
            if deal.link not in combined_results:
                combined_results[deal.link] = {
                    "content": f"[{deal.source}] {deal.title} - ê°€ê²©: {deal.price}",
                    "link": deal.link,
                    "source": "í‚¤ì›Œë“œë§¤ì¹­"
                }
        
        # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë¦¬ìŠ¤íŠ¸ ë³€í™˜)
        final_docs_content = [item["content"] for item in combined_results.values()]
        
        if not final_docs_content:
            return {"answer": "ê´€ë ¨ëœ í•«ë”œì„ ì°¾ì§€ ëª»í–ˆì–´ìš” ğŸ˜¿ (í‚¤ì›Œë“œë‚˜ AIë‚˜ ë‘˜ ë‹¤ ëª¨ë¥¸ëŒ€ìš”!)"}

        # --- 4ë‹¨ê³„: Geminiì—ê²Œ ë‹µë³€ ìš”ì²­ ---
        template = """ë„ˆëŠ” í•«ë”œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ëŠ” ë˜‘ë˜‘í•œ ê³ ì–‘ì´ 'ë”œëƒ¥ì´'ì•¼.
        ì•„ë˜ëŠ” 'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ'ì´ ì°¾ì•„ë‚¸ í•«ë”œ ëª©ë¡ì´ì•¼.
        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë‹µë³€í•´ì¤˜.
        
        [ê²€ìƒ‰ëœ í•«ë”œ ëª©ë¡]
        {context}
        
        ì‚¬ìš©ì ì§ˆë¬¸: {question}
        
        ë‹µë³€ ê°€ì´ë“œë¼ì¸:
        1. ì§ˆë¬¸í•œ ë¬¼ê±´ê³¼ **ê°€ì¥ ì •í™•í•œ ëª¨ë¸**ì´ ìˆë‹¤ë©´ ê·¸ê±¸ ìµœìš°ì„ ìœ¼ë¡œ ì¶”ì²œí•´.
        2. ìƒí’ˆëª…, ê°€ê²©, ì‡¼í•‘ëª°(ì¶œì²˜)ë¥¼ ëª…í™•íˆ ì–¸ê¸‰í•´.
        3. ëª©ë¡ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  ì—†ë‹¤ê³  ë§í•´.
        4. ë§íˆ¬ëŠ” ì¹œì ˆí•œ ê³ ì–‘ì´ ë§íˆ¬('~ì´ë‹¤ëƒ¥', '~í–ˆë‹¤ëƒ¥')ë¥¼ ì¨ì¤˜.
        """
        prompt = ChatPromptTemplate.from_template(template)
        
        model = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash", 
            temperature=0, 
            google_api_key=GOOGLE_API_KEY,
            transport="rest"
        )
        
        chain = (
            {"context": lambda x: "\n".join(final_docs_content), "question": RunnablePassthrough()}
            | prompt
            | model
            | StrOutputParser()
        )
        
        response = chain.invoke(query)
        
        # í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš© ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
        sources = [{"title": item["content"], "link": item["link"]} for item in combined_results.values()]
        
        return {
            "answer": response,
            "sources": sources
        }
        
    except Exception as e:
        logger.error(f"AI ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"answer": f"ì£„ì†¡í•´ìš”, ì¸„ë¥´ë¥¼ ë¨¹ëŠë¼ ë‹µë³€ì„ ëª»í–ˆì–´ìš” ğŸ˜¿ ({str(e)})"}

# í˜„ì¬ ìœ ì € ì •ë³´ ì¡°íšŒ
@app.get('/api/auth/me')
async def get_me(current_user: User = Depends(get_current_user_required)):
    """í˜„ì¬ ë¡œê·¸ì¸í•œ ìœ ì € ì •ë³´"""
    return {
        "id": current_user.id,
        "username": current_user.username,
        "email": current_user.email,
        "provider": current_user.provider,
        "profile_image": current_user.profile_image,
        "created_at": current_user.created_at.strftime('%Y-%m-%d')
    }

@app.get('/api/auth/naver/callback')
async def naver_callback(code: str, state: str, db: Session = Depends(get_db)):
    """ë„¤ì´ë²„ ë¡œê·¸ì¸ ì½œë°± ì²˜ë¦¬"""
    
    print(f"ğŸ”µ ë„¤ì´ë²„ ì½œë°± ì‹œì‘: code={code[:10]}...")
    
    # ... (í† í° ë°œê¸‰ ì½”ë“œ)
    
    print(f"âœ… ë„¤ì´ë²„ ì•¡ì„¸ìŠ¤ í† í°: {access_token[:20]}...")
    
    # ... (ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°)
    
    print(f"âœ… ë„¤ì´ë²„ ì‚¬ìš©ì ì •ë³´: {naver_user}")
    
    # ... (DB ì €ì¥)
    
    print(f"âœ… ìœ ì € ìƒì„±/ì¡°íšŒ ì™„ë£Œ: {user.username}")
    
    # JWT í† í° ìƒì„±
    jwt_token = create_access_token(data={"sub": user.id})
    print(f"âœ… JWT í† í° ìƒì„±: {jwt_token[:30]}...")
    
    # í”„ë¡ íŠ¸ì—”ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    frontend_url = f"http://localhost:8000/?token={jwt_token}"
    
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url=frontend_url)

# --- [ê´€ë¦¬ììš©] DB ê°•ì œ ë™ê¸°í™” API ---
@app.get("/api/admin/sync-rag")
async def sync_rag_manually(db: Session = Depends(get_db)):
    """ê¸°ì¡´ DBì˜ ë°ì´í„°ë¥¼ ë²¡í„° DBë¡œ ê°•ì œ ì´ì‹"""
    if not GOOGLE_API_KEY:
        return {"status": "error", "message": "Google API Key ì—†ìŒ"}
    
    try:
        # 1. ëª¨ë“  í•«ë”œ ê°€ì ¸ì˜¤ê¸°
        all_deals = db.query(HotDeal).all()
        if not all_deals:
            return {"status": "empty", "message": "DBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
            
        # 2. ë²¡í„° ë¬¸ì„œë¡œ ë³€í™˜
        documents = []
        for deal in all_deals:
            doc = Document(
                page_content=f"[{deal.source}] {deal.title} - ê°€ê²©: {deal.price}",
                metadata={"link": deal.link, "source": deal.source, "price": deal.price}
            )
            documents.append(doc)
            
        # 3. ë²¡í„° DBì— ì €ì¥
        vectorstore = get_vectorstore()
        if vectorstore:
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì´ˆê¸°í™”ê°€ ì¢‹ê² ì§€ë§Œ, 
            # ì¼ë‹¨ ë®ì–´ì“°ê±°ë‚˜ ì¶”ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ (ChromaëŠ” ID ì—†ìœ¼ë©´ ì¶”ê°€ë¨)
            vectorstore.add_documents(documents)
            
        return {"status": "success", "message": f"ì´ {len(documents)}ê°œì˜ í•«ë”œì„ AIì—ê²Œ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤!"}
        
    except Exception as e:
        logger.error(f"ë™ê¸°í™” ì‹¤íŒ¨: {e}")
        return {"status": "error", "message": str(e)}


# ì •ì  íŒŒì¼ ì œê³µ
current_dir = os.path.dirname(os.path.abspath(__file__))
templates_dir = os.path.join(current_dir, "templates")

# main.js ì§ì ‘ ì„œë¹™ ì¶”ê°€
@app.get("/main.js")
async def serve_main_js():
    file_path = os.path.join(templates_dir, "main.js")
    return FileResponse(file_path, media_type="application/javascript")

@app.get("/", response_class=FileResponse)
async def read_root():
    return os.path.join(templates_dir, "index.html")

#app.mount("/", StaticFiles(directory=templates_dir, html=True), name="static")

# ìƒˆ ì½”ë“œ (Railwayì—ì„œëŠ” ì‚¬ìš© ì•ˆ í•¨)
if __name__ == '__main__':
    import sys
    port = int(os.getenv("PORT", 8000))
    print(f"ğŸš€ ë¡œì»¬ ì„œë²„ ì‹œì‘: http://localhost:{port}")
    uvicorn.run(app, host="0.0.0.0", port=port, reload=True)
